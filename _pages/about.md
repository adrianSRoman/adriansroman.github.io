---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
#  more_info: >
#    <p>555 your office number</p>
#    <p>123 your address street</p>
#    <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am an Audio Software Engineer at [Tesla](https://www.tesla.com/about) and a M.S. student in Computer Science at the University of Southern California. 

My research focuses on developing explainable AI algorithms for multi-modal machine perception, with a particular emphasis on building auditory models to detect and localize sound events. I believe machine listening can significantly enhance the capabilities of intelligent systems, driving innovation in areas like robotics, autonoumous vehicles, and beyond. 

As a researcher and technologist, my main goal is to develop innovation that can ultimately improve people's lives, especially those with disabilities. At [Oscillo Biosciences](https://oscillobiosciences.com/about/), I developed non-linear dynamical systems to simulate human synchronization. I delivered a mobile digital therapy capable of gently re-training rhytmic synchronization on patients who suffer from language disfluencies and aphasia. 

At Tesla I develop sound user interfaces. By using sounds, I help humans navigate their vehicles. My contributions also expand other areas of the software stack, such as UI development, audio pipelines, and firmware. Beyond core audio software engineering, I build machine learning systems for speech enhacement and sound event localization and detection. 
