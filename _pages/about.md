---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
#  more_info: >
#    <p>555 your office number</p>
#    <p>123 your address street</p>
#    <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am an Audio Software Engineer at [Tesla](https://www.tesla.com/about) and a M.S. student in Computer Science at the [University of Southern California](https://www.cs.usc.edu/). 

My research centers on developing explainable AI algorithms for multi-modal machine perception, with a particular emphasis on auditory models for sound event localization and detection (SELD). I believe auditory perception is crucial for creating robust, multi-modal embodied agents. 

As a technologist, my core intention is to innnovative with ideas materialized into products that ultimately improve people's lives, especially those with disabilities. At [Oscillo Biosciences](https://oscillobiosciences.com/about/), I developed non-linear dynamical systems to simulate human synchronization. I delivered a mobile digital therapy capable of gently re-training rhytmic synchronization on patients who suffer from language disfluencies and aphasia. 

As an engineer, at Tesla I develop sound user interfaces. By using sounds, I help humans navigate their vehicles. My contributions also expand other areas of the software stack, such as UI development, audio pipelines, and firmware. Beyond core audio software engineering, I build machine learning systems for speech enhacement and sound event localization and detection. 
